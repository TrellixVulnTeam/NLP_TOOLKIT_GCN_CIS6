{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GATtry.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTl6j2ZFxAwQSS8dNGhbYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ridhi26/NLP_TOOLKIT_GCN/blob/master/GATtry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpOam601PaFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agvI8mHCR2kU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "22e1b20c-e190-470b-9c0c-06b8b86aab86"
      },
      "source": [
        "!git clone https://github.com/ridhi26/NLP_TOOLKIT_GCN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_TOOLKIT_GCN'...\n",
            "remote: Enumerating objects: 320, done.\u001b[K\n",
            "remote: Counting objects: 100% (320/320), done.\u001b[K\n",
            "remote: Compressing objects: 100% (219/219), done.\u001b[K\n",
            "remote: Total 320 (delta 98), reused 290 (delta 86), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (320/320), 5.62 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (98/98), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj688PJ7R5ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1132d240-cbe9-440f-a4b3-78a45a08d393"
      },
      "source": [
        "cd /content/NLP_TOOLKIT_GCN\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NLP_TOOLKIT_GCN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5dfKuvSDTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee515bab-a020-43b4-d94e-c67a95758c0f"
      },
      "source": [
        "! pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/NLP_TOOLKIT_GCN\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dask==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/5e/45d829bbb3c7489b8b0b77109bbd0b248732973a6b3cb65f54ccaf712c5e/dask-2.1.0-py3-none-any.whl (763kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 3.4MB/s \n",
            "\u001b[?25hCollecting fairseq==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/19/a71af3ea3bdf7c2fd66c8076a3be090147f700e3e513b0b3b11d80d97fe3/fairseq-0.8.0.tar.gz (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 51.6MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.34\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/04/b92425ca552116afdb7698fa3f00ca1c975cfd86a847cf132fd813c5d901/sacremoses-0.0.34.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 45.8MB/s \n",
            "\u001b[?25hCollecting pandas==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 38.7MB/s \n",
            "\u001b[?25hCollecting kenlm==0.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/54/0cc492b8d7aceb17a9164c6e6b9c9afc2c73706bb39324e8f6fa02f7134a/kenlm-0.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 46.5MB/s \n",
            "\u001b[?25hCollecting soundfile==0.10.2\n",
            "  Downloading https://files.pythonhosted.org/packages/68/64/1191352221e2ec90db7492b4bf0c04fd9d2508de67b3f39cbf093cd6bd86/SoundFile-0.10.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto==2.49.0 in /usr/local/lib/python3.6/dist-packages (from nlptoolkit==0.0.18) (2.49.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.6MB/s \n",
            "\u001b[?25hCollecting jieba==0.39\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 40.2MB/s \n",
            "\u001b[?25hCollecting tqdm==4.32.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.83\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.3MB/s \n",
            "\u001b[?25hCollecting fasttext==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting librosa==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/6e/0eb0de1c9c4e02df0b40e56f258eb79bd957be79b918511a184268e01720/librosa-0.7.0.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 46.5MB/s \n",
            "\u001b[?25hCollecting networkx==2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz==0.10.0 in /usr/local/lib/python3.6/dist-packages (from nlptoolkit==0.0.18) (0.10.0)\n",
            "Collecting nltk>=3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 45.1MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting boto3==1.9.238\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/7c/049e174ffc12da9003ade818eb3a569681418357082f12266cdaa08df52f/boto3-1.9.238-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from nlptoolkit==0.0.18) (3.0.12)\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 236kB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 172kB/s \n",
            "\u001b[?25hCollecting allennlp==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 41.9MB/s \n",
            "\u001b[?25hCollecting torchtext==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/58/f7b07a9eb4a39adeea22c46554a51432785fe5812770334c208da926577f/sacrebleu-1.4.1-py3-none-any.whl\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[?25hCollecting pytorch-nlp==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ae/b6d18c3f37da5a78e83701469e6153811f4b0ecb3f9387bb3e9a65ca48ee/pytorch_nlp-0.4.1-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.5MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 31.3MB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 38.5MB/s \n",
            "\u001b[?25hCollecting python-Levenshtein==0.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting seqeval==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Collecting spacy==2.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/9c/afd55bb35cc03e4b3dadc41dd48bc26e0678b08d59f32411735c35bda550/spacy-2.1.8-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 127kB/s \n",
            "\u001b[?25hCollecting regex==2019.8.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq==0.8.0->nlptoolkit==0.0.18) (1.14.2)\n",
            "Collecting fastBPE\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.34->nlptoolkit==0.0.18) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.34->nlptoolkit==0.0.18) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->nlptoolkit==0.0.18) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->nlptoolkit==0.0.18) (2018.9)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->nlptoolkit==0.0.18) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->nlptoolkit==0.0.18) (49.6.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.0->nlptoolkit==0.0.18) (2.1.8)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.0->nlptoolkit==0.0.18) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.0->nlptoolkit==0.0.18) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.0->nlptoolkit==0.0.18) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.0->nlptoolkit==0.0.18) (0.48.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3==1.9.238->nlptoolkit==0.0.18) (0.10.0)\n",
            "Collecting s3transfer<0.3.0,>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.238\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/7b/88f10115b4748f86be6b7b1d8761ba5023fccf6e6cbe762e368f63eddcf9/botocore-1.12.253-py2.py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8MB 38.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->nlptoolkit==0.0.18) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->nlptoolkit==0.0.18) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->nlptoolkit==0.0.18) (1.2.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 38.6MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 46.3MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (0.3.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (0.5.3)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/18/3932900a42d7010cc63529fc5cb7a5a20fb61878d1721d0d4387567d5973/gevent-20.6.2-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 36.6MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/04/8a5258cfd851c9c89ae5c12c6952c05d42ca8c0788b999806e0c78d06c54/responses-0.12.0-py2.py3-none-any.whl\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.4MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 51.3MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (2.10.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (1.1.2)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (3.6.4)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0->nlptoolkit==0.0.18) (2.23.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting typing\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->nlptoolkit==0.0.18) (7.0.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12->nlptoolkit==0.0.18) (2.4.3)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.8->nlptoolkit==0.0.18) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.8->nlptoolkit==0.0.18) (1.0.2)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.8->nlptoolkit==0.0.18) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.8->nlptoolkit==0.0.18) (0.7.1)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 25.1MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq==0.8.0->nlptoolkit==0.0.18) (2.20)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa==0.7.0->nlptoolkit==0.0.18) (0.31.0)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.238->boto3==1.9.238->nlptoolkit==0.0.18) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.238->boto3==1.9.238->nlptoolkit==0.0.18) (0.15.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0->nlptoolkit==0.0.18) (3.12.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==0.9.0->nlptoolkit==0.0.18) (0.2.5)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 50.4MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Collecting greenlet>=0.4.16; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9.0->nlptoolkit==0.0.18) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9.0->nlptoolkit==0.0.18) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==0.9.0->nlptoolkit==0.0.18) (2.11.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->nlptoolkit==0.0.18) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->nlptoolkit==0.0.18) (8.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->nlptoolkit==0.0.18) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->nlptoolkit==0.0.18) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==0.9.0->nlptoolkit==0.0.18) (20.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==0.9.0->nlptoolkit==0.0.18) (1.7.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (1.8.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9.0->nlptoolkit==0.0.18) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9.0->nlptoolkit==0.0.18) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==0.9.0->nlptoolkit==0.0.18) (3.0.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->nlptoolkit==0.0.18) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9.0->nlptoolkit==0.0.18) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp==0.9.0->nlptoolkit==0.0.18) (3.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (1.2.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (2.1.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (1.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (2.8.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (2.0.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (20.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0->nlptoolkit==0.0.18) (1.1.4)\n",
            "Building wheels for collected packages: nlptoolkit\n",
            "  Building wheel for nlptoolkit (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlptoolkit: filename=nlptoolkit-0.0.18-cp36-none-any.whl size=4445222 sha256=f928131f9850ec5282cd7da031dc1c098546ed45da37b54f1190344bec9fc732\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iks6myc9/wheels/84/ce/20/51893e605e5535d1ef7d3cf808273b6d72ec95033c59f65cb2\n",
            "Successfully built nlptoolkit\n",
            "Building wheels for collected packages: fairseq, sacremoses, kenlm, jieba, fasttext, librosa, networkx, nltk, python-Levenshtein, seqeval, regex, fastBPE, word2number, ftfy, jsonnet, overrides, parsimonious, typing\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.8.0-cp36-cp36m-linux_x86_64.whl size=363811 sha256=e3fa91a520bef9df6fc069d919830e537ec4d3555833f60601721ae19a85a259\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/30/0b/92e3a8eff3c07c40049b517e3333dfab0283a41f2b6b2cc9be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.34-cp36-none-any.whl size=883992 sha256=baec9265f8aab863d27bce5114a84a5da6e609927161aafab7eaa3ed67ae9915\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/b9/5b/8bd674c23e962fbff34420a9fa7a2c374d591ecadd5bc37684\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2271913 sha256=835882c0dca5647d1aee1bb9cb5bbe16dee01550dc7f197398c390af4268096d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/cf/f4/1a1aab56f87f4132667a7a47045a750384f19d646099ab4858\n",
            "  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba: filename=jieba-0.39-cp36-none-any.whl size=7282594 sha256=3e593d435e030701711aa895c3ad463a9a8e537c373bcbd8f9dc9ba32052f2b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c7/63/a9ec0322ccc7c365fd51e475942a82395807186e94f0522243\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2386177 sha256=b1cbcb77cc4a5bb4326a8277e7a842f9258e391500edb33ad35e84a24e79da58\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.0-cp36-none-any.whl size=1598345 sha256=ac62fbacb06853ed50691d26de1178a0502e71dedbe425896469dbf6d0dc1579\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/1d/38/c8ad12fcad67569d8e730c3275be5e581bd589558484a0f881\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556409 sha256=5f66821ae4137fe0f5092a7828e03151ce52b5920f8f33da96256477f5fc1abc\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434675 sha256=f0b967f5173685e912f785a98e505f1be4701bd301a8b2b66b2f2dbf4bbbd56a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144809 sha256=9d4c3202d43684a7d0999af917a09eaa631e84d7de9a33b221cf5c05e2eba610\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=daa73f91d315795f674d3e932155448830bd76dde6b87e386ab4798087782f0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609246 sha256=e14ca6b64700d9d9dc8ba0583902e745370f96a941d3ba41fc23553d878f045c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481488 sha256=f35cf9441445594d02ed8852752a239dc4fd1f32da0899106b03740f7b341ba1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=b8299c05740b85deb132326172964990ed9f1f17050674b17fa2aba21a7f15d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=ce22cde6513046cb8ca19e37a93d23d8d96162e02447389ad96a7953ed582c95\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321566 sha256=d9676943908ac2adeb0483d4a9a1cf2ae4c37399ec30f6439b2fd378fc9c0b8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=f8c2f32eb15cd73dd4697e40e5d9c4bee36334b5a5c340abf664f7701715eb43\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=e9e6a04200e5d62c331b62e99a790bb18c5abe01e67960096f1fdd017e46ba30\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp36-none-any.whl size=26307 sha256=d7b4f0cec93e4afa1a98d4654119eba9b07f948b28439bb8efc24711910c3c79\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "Successfully built fairseq sacremoses kenlm jieba fasttext librosa networkx nltk python-Levenshtein seqeval regex fastBPE word2number ftfy jsonnet overrides parsimonious typing\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: responses 0.12.0 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: dask, fastBPE, numpy, regex, portalocker, typing, sacrebleu, torch, tqdm, fairseq, six, sacremoses, pandas, kenlm, soundfile, tokenizers, jieba, sentencepiece, fasttext, scikit-learn, librosa, networkx, nltk, botocore, s3transfer, boto3, matplotlib, tensorboardX, pytorch-transformers, word2number, ftfy, plac, preshed, blis, thinc, spacy, zope.interface, zope.event, greenlet, gevent, responses, unidecode, pytorch-pretrained-bert, jsonnet, flask-cors, overrides, parsimonious, jsonpickle, conllu, flaky, numpydoc, allennlp, torchtext, pytorch-nlp, torchvision, python-Levenshtein, seqeval, nlptoolkit\n",
            "  Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Found existing installation: jieba 0.42.1\n",
            "    Uninstalling jieba-0.42.1:\n",
            "      Successfully uninstalled jieba-0.42.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "  Found existing installation: networkx 2.5\n",
            "    Uninstalling networkx-2.5:\n",
            "      Successfully uninstalled networkx-2.5\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: botocore 1.17.48\n",
            "    Uninstalling botocore-1.17.48:\n",
            "      Successfully uninstalled botocore-1.17.48\n",
            "  Found existing installation: s3transfer 0.3.3\n",
            "    Uninstalling s3transfer-0.3.3:\n",
            "      Successfully uninstalled s3transfer-0.3.3\n",
            "  Found existing installation: boto3 1.14.48\n",
            "    Uninstalling boto3-1.14.48:\n",
            "      Successfully uninstalled boto3-1.14.48\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 boto3-1.9.238 botocore-1.12.253 conllu-1.3.1 dask-2.1.0 fairseq-0.8.0 fastBPE-0.1.0 fasttext-0.9.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.6.2 greenlet-0.4.16 jieba-0.39 jsonnet-0.16.0 jsonpickle-1.4.1 kenlm-0.0.0 librosa-0.7.0 matplotlib-3.1.0 networkx-2.3 nlptoolkit-0.0.18 nltk-3.5 numpy-1.16.4 numpydoc-1.1.0 overrides-3.1.0 pandas-0.24.2 parsimonious-0.8.1 plac-0.9.6 portalocker-2.0.0 preshed-2.0.1 python-Levenshtein-0.12.0 pytorch-nlp-0.4.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 regex-2019.8.19 responses-0.12.0 s3transfer-0.2.1 sacrebleu-1.4.1 sacremoses-0.0.34 scikit-learn-0.21.2 sentencepiece-0.1.83 seqeval-0.0.12 six-1.12.0 soundfile-0.10.2 spacy-2.1.8 tensorboardX-2.1 thinc-7.0.8 tokenizers-0.7.0 torch-1.4.0 torchtext-0.4.0 torchvision-0.5.0 tqdm-4.32.1 typing-3.7.4.3 unidecode-1.1.1 word2number-1.1 zope.event-4.4 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas",
                  "six",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9hJf4qHSGbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4c0c1358-0151-4683-9291-a700d7df07b0"
      },
      "source": [
        "! python -m spacy download en_core_web_lg\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 1.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255078 sha256=2f30897ecbe9c4e1b156ad245daed82091dcc72a58ea765db5e3fa0a7451c9cc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qvpuc_5b/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r60tQnSlT5VA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "22ecd0b8-b440-4eaa-f5e3-1cffeb173834"
      },
      "source": [
        "from nlptoolkit.utils.config import Config\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/10/2020 09:22:18 AM [INFO]: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "09/10/2020 09:22:18 AM [INFO]: PyTorch version 1.4.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: TensorFlow version 2.3.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: PyTorch version 1.4.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: TensorFlow version 2.3.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "09/10/2020 09:22:23 AM [INFO]: TensorFlow version 2.3.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: PyTorch version 1.4.0 available.\n",
            "09/10/2020 09:22:23 AM [INFO]: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "09/10/2020 09:22:23 AM [INFO]: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re86I9AOUoye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlptoolkit.classification.models.GCN.trainer import train_and_fit\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B04YlIgOUsSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlptoolkit.classification.models.infer import infer_from_trained\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C86NhZOOUunG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config(task='classification') # loads default argument parameters as above\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNITd_RaUwox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.train_data = '/content/NLP_TOOLKIT_GCN/data/train.csv' # sets training data path\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiHukJ3OUy0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.infer_data = '/content/NLP_TOOLKIT_GCN/data/infer.csv' # sets infer data path\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8wnYPT_U0mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.num_classes = 2 # sets number of prediction classes\n",
        "config.batch_size = 4 # Changed batch size to 4\n",
        "config.model_no = 3 # sets GCN model\n",
        "config.lr = 0.001 # change learning rate\n",
        "config.train_test_split = 1\n",
        "config.infer=1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9l3J9d0U_BQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "45f43595-3acc-4f07-c033-4c6e6822b055"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIQVMV-sVC17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "ef8cc542-bf48-4f2e-a408-c8c0044878a1"
      },
      "source": [
        "train_and_fit(config) # starts training with configured parameters\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/10/2020 09:24:08 AM [INFO]: Loading data...\n",
            "09/10/2020 09:24:08 AM [INFO]: Building datasets and graph from raw data... Note this will take quite a while...\n",
            "09/10/2020 09:24:08 AM [INFO]: Preparing data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cf025adbb988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# starts training with configured parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nlptoolkit/classification/models/GCN/trainer.py\u001b[0m in \u001b[0;36mtrain_and_fit\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_not_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#print(labels_selected, labels_not_selected)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nlptoolkit/classification/models/GCN/train_funcs.py\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m(args, train_test_split)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building datasets and graph from raw data... Note this will take quite a while...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mgenerate_text_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_vocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_data.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mG_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_graph.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nlptoolkit/classification/models/GCN/preprocessing_funcs.py\u001b[0m in \u001b[0;36mgenerate_text_graph\u001b[0;34m(train_data, infer_data, max_vocab_len, window)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m### tokenize & remove funny characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfilter_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0msave_as_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m### Tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nlptoolkit/classification/models/GCN/preprocessing_funcs.py\u001b[0m in \u001b[0;36msave_as_pickle\u001b[0;34m(filename, data)\u001b[0m\n\u001b[1;32m     32\u001b[0m     completeName = os.path.join(\"./data/\",\\\n\u001b[1;32m     33\u001b[0m                                 filename)\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompleteName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/df_data.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPMXVed8VEw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}